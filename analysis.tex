\chapter{Analyzing Algorithms}
Or we would look at the list, but we need to talk about Math.  Sorry for the bait-and-switch, but it will make sense shortly.


You don't need  much math to be a good programmer, but if you want to be an amazing programmer, you probably need math or very math adjacent skills.
\section{Cost}
Every function, operation, algorithm, or what have you that a computer performs has a \emph{cost}. 
In fact, there are always multiples costs;  we often just focus on the most important one or two costs.  

What is most important depends on context.
However, in the vast majority of cases, the most important cost to focus on is \textbf{time}.
When our program is eating away at our storage resources like a hungry child slurping up spaghetti, we can always go out and buy more memory/storage/RAM.
If our program requires a large amount of energy consumption, energy is readily available from a variety of sources: batteries, power plugs, internal combustion engines, the giant fusion reactor in the sky.




\subsubsection*{Measuring Cost}
When we measure cost, we need to do abstractly.  
When we measure the amount of time that an algorithm takes, we look at the number of operations that will be executed, not the overall elapsed time.

\subsection{Time}
A time cost is a measure of not just how long it takes a program to finish executing, bit also how the length of execution is affected by adding additional item.

Time is almost always \emph{the most important cost}. We cannot got out and buy another weeks worth of time.  You can't hand a bunch of money to the reaper and ask for a deferral. You can't buy another minute to spend with your mother\footnote{Call your mother.  She would love to hear from you.} 
Yes, processors get faster as technology marches on, but they get faster slowly and Moore's law ostensibly has its limits.
The only way to make our programs realistically run faster is to make them more efficient.  And \textbf{Big O notation} is the way we will be measuring that efficiency.


\subsection{Space}

For data structures, we will be measuring their space efficiency in terms of \textit{auxiliary} cost, in other words, how much extra space we need to use over the space used for the items itself.   To clarify, any data structure that contains $ n $ items of roughly the same size will use $ n \times \mathtt{sizeOf(item)}$ space at minimum, no matter what data structure we use.  This accounts for the 
Each data structures 

\subsection{Energy}
%\subsection{Other costs - Bandwidth}
While not something this book concerns itself with, some programmers need to be wary of the amount of energy  an algorithm consumes.  If energy is expensive and/or battery life needs to be conserved, then choosing an energy efficient algorithm might be a better choice, even if the time or space complexity is higher.  Some examples where energy use is a large concern  is Mobile Ad-Hoc Networks (MANETs) and battery powered cameras.


\section{Big O Notation}

\begin{itemize}
	\item What is big O
	
	\item  how to read it
	\item Aside about big omega and theta
	\item How wrong usage annoys mathematician
	\item refers to cost in general, but used for time usually
	\item  space complexity 
	\item Common runtimes
	\item runtimes we''ll focus on now
	\item runtimes we focus on later
\end{itemize}



\subsection{Space Complexity}

\section{Examples with Arrays}

\begin{itemize}
	
	\item Retrieval  - refer back to earlier chapter for address lookup 
	\item Replacement
	\item Linear Search
	\item Binary Search
\end{itemize}



\subsection{Selection Sort}



\subsection{Bubble Sort}
\subsection{Insertion Sort}
\subsection{Other Sorting Algorithms}


\section{The Formal Mathematics of Big O Notation}
\section{Other Notations}


\section{When To Ignore Costs}